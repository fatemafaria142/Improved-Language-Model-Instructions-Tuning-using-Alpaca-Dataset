<h1>Improved-Language-Model-Instructions-Tuning-using-Alpaca-Dataset</h1>

<p>In this project, I explored different prompt types for Large Language Models.</p>

<h2>Alpaca Dataset</h2>
<p>I utilized the "Alpaca" dataset, which comprises 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine. This instruction data is ideal for conducting instruction-tuning for language models, enhancing their ability to follow instructions effectively.</p>
<p>Dataset Link: <a href="https://huggingface.co/datasets/tatsu-lab/alpaca">Alpaca Dataset</a></p>

<h2>Large Language Models (LLMs)</h2>
<p>I employed six different types of Large Language Models for this task. Here are the details along with their respective links:</p>

<ol>
  <li>
    <h3>GPT2</h3>
    <p>Model Link: <a href="https://huggingface.co/docs/transformers/model_doc/gpt2">GPT2 Documentation</a></p>
  </li>
  <li>
    <h3>GPT-Medium</h3>
    <p>Model Link: <a href="https://huggingface.co/openai-community/gpt2-medium">GPT-Medium</a></p>
  </li>
  <li>
    <h3>Mistral-7B-v0.1</h3>
    <p>Model Link: <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1">Mistral-7B-v0.1</a></p>
  </li>
  <li>
    <h3>TinyLlama-1.1B-Chat-v1.0</h3>
    <p>Model Link: <a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0">TinyLlama-1.1B-Chat-v1.0</a></p>
  </li>
   <li>
    <h3>Mistral-7B-Instruct-v0.2</h3>
    <p>Model Link: <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">Mistral-7B-Instruct-v0.2</a></p>
  </li>
   <li>
    <h3>Starling-LM-7B-alpha</h3>
    <p>Model Link: <a href="https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha">Starling-LM-7B-alpha</a></p>
  </li>
</ol>

<p>Feel free to explore these models and the Alpaca dataset for a deeper understanding of the project's advancements in language model instruction tuning.</p>
