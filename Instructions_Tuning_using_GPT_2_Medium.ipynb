{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatemafaria142/Improved-Language-Model-Instructions-Tuning-using-Alpaca-Dataset/blob/main/Instructions_Tuning_using_GPT_2_Medium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6LzHgS_Fkh5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "mNz5iKXhFug-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "ENBd58R5Ft_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "gXFW8F5HFuEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "HZyi2-_mFuJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Link:** https://huggingface.co/datasets/tatsu-lab/alpaca?row=0"
      ],
      "metadata": {
        "id": "hka_zLJTff6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\")"
      ],
      "metadata": {
        "id": "5FVft-5zF_f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "5fpMKflMF_i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nFm7F9YMGFe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get the first 5000 data points**"
      ],
      "metadata": {
        "id": "m2wHOnhdfs_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first 5000 data points\n",
        "num_samples_to_display = 5000\n",
        "subset_dataset = dataset['train'].select(range(num_samples_to_display))\n",
        "\n",
        "# Display information for 3 data points from the subset\n",
        "num_samples_to_show = 3\n",
        "for i in range(num_samples_to_show):\n",
        "    data = subset_dataset[i]\n",
        "    print(f\"Data Point {i + 1}:\")\n",
        "    print(\"Instruction:\", data['instruction'])\n",
        "    print(\"Input:\", data['input'])\n",
        "    print(\"Output:\", data['output'])\n",
        "    print(\"Text:\", data['text'])\n",
        "    print(\"\\n-----------------------------\\n\")"
      ],
      "metadata": {
        "id": "m5omug3zGRnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions to Generate Prompts from Dataset Entries**"
      ],
      "metadata": {
        "id": "4T31f3CMfuzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt_with_input(x):\n",
        "    result = f\"### Instruction:\\n{x['instruction']}\\n\\n\"\n",
        "    result += f\"### Input:\\n{x['input']}\\n\\n\"\n",
        "    result += f\"### Response:\\n{x['output']}\"\n",
        "    return result\n",
        "\n",
        "def get_prompt_without_input(x):\n",
        "    result = f\"### Instruction:\\n{x['instruction']}\\n\\n\"\n",
        "    result += f\"### Response:\\n{x['output']}\"\n",
        "    return result\n",
        "\n",
        "def get_prompt(x):\n",
        "    if x['input'] == '':\n",
        "        return get_prompt_without_input(x)\n",
        "    else:\n",
        "        return get_prompt_with_input(x)\n"
      ],
      "metadata": {
        "id": "5gC0JXSyHaVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Display prompts for the first 5 data points**"
      ],
      "metadata": {
        "id": "570EBoeugECb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate prompts for each data point in the subset dataset\n",
        "prompts = []\n",
        "for i in range(num_samples_to_display):\n",
        "    data = subset_dataset[i]\n",
        "    prompt = get_prompt(data)\n",
        "    prompts.append(prompt)\n",
        "\n",
        "# Display the generated prompts or use them as needed\n",
        "for idx, prompt in enumerate(prompts[:5]):  # Display prompts for the first 5 data points\n",
        "    print(f\"Prompt for Data Point {idx + 1}:\")\n",
        "    print(prompt)\n",
        "    print(\"\\n-----------------------------\\n\")\n"
      ],
      "metadata": {
        "id": "8deQRMNlJcAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **More Examples of Prompts**"
      ],
      "metadata": {
        "id": "wIV0IPUNgIrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate prompts for each data point in the subset dataset\n",
        "prompts = []\n",
        "for i in range(num_samples_to_display):\n",
        "    data = subset_dataset[i]\n",
        "    prompt = get_prompt(data)\n",
        "    prompts.append(prompt)\n",
        "\n",
        "# Display the generated prompts or use them as needed\n",
        "for idx, prompt in enumerate(prompts[5:10]):  # Display prompts for the first 3 data points\n",
        "    print(f\"Prompt for Data Point {idx + 1}:\")\n",
        "    print(prompt)\n",
        "    print(\"\\n-----------------------------\\n\")\n"
      ],
      "metadata": {
        "id": "jIA4PQZEPjxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT-2 Medium and its tokenizer**\n",
        "* https://huggingface.co/openai-community/gpt2-medium"
      ],
      "metadata": {
        "id": "ARL-NrpAgSSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2-medium')"
      ],
      "metadata": {
        "id": "XBkjQjqojmyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the padding token for the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "N3rmJTbDjy6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preparation and Tokenization for GPT-2 Training**\n",
        "* This code segment appears to perform the following tasks:\n",
        "\n",
        "* Assumes a dataset with specific keys ('instruction', 'input', 'output', and 'text').\n",
        "* Iterates through the dataset to generate prompts based on the available data points.\n",
        "* Tokenizes the generated prompts and output text using a tokenizer, preparing them as tensors.\n",
        "* Compiles tokenized inputs, labels, and attention masks to be used for GPT-2 model training."
      ],
      "metadata": {
        "id": "YQ_YQmJZg1GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "# Assuming subset_dataset is your dataset with 'instruction', 'input', 'output', and 'text' keys\n",
        "dataset = subset_dataset  # No need for ['train'] if keys are 'instruction', 'input', 'output', and 'text'\n",
        "\n",
        "# Initialize empty lists to store inputs, targets, and attention masks\n",
        "input_ids = []\n",
        "labels = []\n",
        "attention_masks = []\n",
        "\n",
        "# Tokenize and prepare the dataset\n",
        "for data_point in dataset:\n",
        "    # Generate prompt\n",
        "    prompt = get_prompt(data_point)\n",
        "\n",
        "    # Tokenize prompt\n",
        "    tokenized_prompts = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    # Tokenize output text (corrected text)\n",
        "    tokenized_output = tokenizer(data_point['output'], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    # Append tokenized inputs, labels, and attention masks\n",
        "    input_ids.append(tokenized_prompts['input_ids'])\n",
        "    labels.append(tokenized_output['input_ids'])\n",
        "    attention_masks.append(tokenized_prompts['attention_mask'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "input_ids = torch.stack(input_ids)\n",
        "labels = torch.stack(labels)\n",
        "attention_masks = torch.stack(attention_masks)"
      ],
      "metadata": {
        "id": "sZ3qkj3RQRix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Printing input_ids, labels, and attention_masks for the first 5 examples**"
      ],
      "metadata": {
        "id": "fV2lLzKngnOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the code provided earlier to prepare the dataset is already executed\n",
        "\n",
        "# Printing input_ids, labels, and attention_masks for the first 5 examples\n",
        "for i in range(3):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(\"Input IDs:\", input_ids[i])\n",
        "    print(\"Labels:\", labels[i])\n",
        "    print(\"Attention Mask:\", attention_masks[i])\n",
        "    print(\"-----------------------\")\n"
      ],
      "metadata": {
        "id": "K7LmtvNYN2Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dynamic Data Collation for GPT-2 Medium Model Training**\n",
        "* This code segment encompasses a class, GPT2DataCollator, which is designed to handle the collation of input features for GPT-2 model training. It dynamically checks the type of input features (whether they are dictionaries or tuples) and appropriately extracts input IDs, attention masks, and labels for padding and preparing the data to the same length. This data collation process is crucial for ensuring the uniformity and compatibility of the input features during the training of the GPT-2 Medium model."
      ],
      "metadata": {
        "id": "VH8dh2xhhAET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "\n",
        "class GPT2DataCollator:\n",
        "    def __call__(self, features):\n",
        "        # Check if the features are dictionaries or tuples\n",
        "        if isinstance(features[0], dict):\n",
        "            input_ids = [feature['input_ids'] for feature in features]\n",
        "            attention_masks = [feature['attention_mask'] for feature in features]\n",
        "            labels = [feature['labels'] for feature in features]\n",
        "        else:  # Assuming features are tuples\n",
        "            input_ids = [feature[0] for feature in features]\n",
        "            attention_masks = [feature[1] for feature in features]\n",
        "            labels = [feature[2] for feature in features]\n",
        "\n",
        "        # Pad inputs and labels to the same length\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_masks,\n",
        "            'labels': labels\n",
        "        }\n"
      ],
      "metadata": {
        "id": "iYwb-UutTO1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define the Training Arguments and Trainer**"
      ],
      "metadata": {
        "id": "uSD-pX5nhYmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./gpt2-finetuned-version',    # Directory to save the model and checkpoints\n",
        "    num_train_epochs=10,               # Number of training epochs\n",
        "    per_device_train_batch_size=4,    # Batch size per device during training\n",
        "    save_steps=1000,                  # Save checkpoint every X steps\n",
        "    logging_dir='./logs',             # Directory for storing logs\n",
        "    logging_steps=500,                # Log training metrics every X steps\n",
        "    evaluation_strategy=\"epoch\",      # Evaluation strategy to adopt during training\n",
        "    report_to=\"none\",                 # Disable evaluation during training\n",
        "    prediction_loss_only=True,        # Compute only the prediction loss\n",
        "    warmup_steps=500# number of warmup steps for learning rate scheduler\n",
        "    # Add any additional arguments as needed\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the custom data collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=torch.utils.data.TensorDataset(input_ids, attention_masks, labels),\n",
        "    eval_dataset=torch.utils.data.TensorDataset(input_ids, attention_masks, labels),\n",
        "    data_collator=GPT2DataCollator()  # Use the custom data collator\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "RHMSmJaATStX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "5HEyY3bxRMEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.save_model()\n"
      ],
      "metadata": {
        "id": "5nZqf496j-pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate perplexity on the test dataset\n",
        "eval_result = trainer.evaluate(eval_dataset=torch.utils.data.TensorDataset(input_ids, attention_masks, labels))\n",
        "print(\"Perplexity:\", eval_result['eval_loss'])"
      ],
      "metadata": {
        "id": "pCsiU9rVPx0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the save model and train it again here...**"
      ],
      "metadata": {
        "id": "pi7ONgjQhifB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from transformers import Trainer, TrainingArguments\n",
        "# Load the previously trained model\n",
        "model_path = './gpt2-finetuned-version'  # Replace this with the path to your saved model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./gpt2-finetuned-version',    # Directory to save the model and checkpoints\n",
        "    num_train_epochs=5,               # Number of training epochs\n",
        "    per_device_train_batch_size=4,    # Batch size per device during training\n",
        "    save_steps=1000,                  # Save checkpoint every X steps\n",
        "    logging_dir='./logs',             # Directory for storing logs\n",
        "    logging_steps=500,                # Log training metrics every X steps\n",
        "    evaluation_strategy=\"epoch\",      # Evaluation strategy to adopt during training\n",
        "    report_to=\"none\",                 # Disable evaluation during training\n",
        "    prediction_loss_only=True,        # Compute only the prediction loss\n",
        "    warmup_steps=500# number of warmup steps for learning rate scheduler\n",
        "    # Add any additional arguments as needed\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the custom data collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=torch.utils.data.TensorDataset(input_ids, attention_masks, labels),\n",
        "    eval_dataset=torch.utils.data.TensorDataset(input_ids, attention_masks, labels),\n",
        "    data_collator=GPT2DataCollator()  # Use the custom data collator\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "IX4xjo1OTUTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Start training\n",
        "trainer.train()\n",
        "'''"
      ],
      "metadata": {
        "id": "DFK04cXTUG_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#Save the fine-tuned model here\n",
        "trainer.save_model()\n",
        "'''"
      ],
      "metadata": {
        "id": "vYtiotAQaTNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Generation Using Fine-Tuned GPT-2 Model Pipeline**"
      ],
      "metadata": {
        "id": "-rcFwCmQhurf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Define the generator pipeline using the fine-tuned GPT-2 model and tokenizer\n",
        "generator = pipeline('text-generation', model='./gpt2-finetuned-version', tokenizer='gpt2')\n",
        "\n",
        "# Example prompt for generating text\n",
        "prompt = \"### Instruction: Give three tips for staying healthy.\"\n",
        "\n",
        "# Generate text based on the prompt using the generator pipeline\n",
        "generated_text = generator(prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "BVC4UIKUYw7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\")"
      ],
      "metadata": {
        "id": "bHpbKb5yP5Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first 10 data points\n",
        "num_samples_to_display = 10\n",
        "subset_dataset = dataset['train'].select(range(num_samples_to_display))\n",
        "\n",
        "# Display information for 3 data points from the subset\n",
        "num_samples_to_show = 10\n",
        "for i in range(num_samples_to_show):\n",
        "    data = subset_dataset[i]\n",
        "    print(f\"Data Point {i + 1}:\")\n",
        "    print(\"Instruction:\", data['instruction'])\n",
        "    print(\"Input:\", data['input'])\n",
        "    print(\"Output:\", data['output'])\n",
        "    print(\"Text:\", data['text'])\n",
        "    print(\"\\n-----------------------------\\n\")"
      ],
      "metadata": {
        "id": "vajtSYzPQFUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Define the generator pipeline using the fine-tuned GPT-2 model and tokenizer\n",
        "generator = pipeline('text-generation', model='./gpt2-finetuned-version', tokenizer='gpt2', pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "# Generate text based on the subset of the training dataset\n",
        "for example in subset_dataset:\n",
        "    test_prompt = (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\r\\n\\r\\n\"\n",
        "        f\"### Instruction:\\r\\n{example['instruction']}\\r\\n\\r\\n### Response:\"  # Replace this with your prompt logic\n",
        "    )\n",
        "\n",
        "    # Generate text based on the test prompt using the generator pipeline\n",
        "    generated_text = generator(test_prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "    # Print the generated text for each test prompt\n",
        "    print(generated_text[0]['generated_text'])\n",
        "    print(\"------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "Y0IuvGGZQFfC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}